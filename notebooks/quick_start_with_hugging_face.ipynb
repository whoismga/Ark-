{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fgVWTMK9SNz"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Quick start with Hugging Face\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/quick_start_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fmedgemma%2Fmain%2Fnotebooks%2Fquick_start_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/quick_start_with_hugging_face.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This notebook provides a basic demo of using MedGemma, a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma is intended to accelerate building healthcare-based AI applications.\n",
        "\n",
        "Learn more about the model at the [HAI-DEF developer site](https://developers.google.com/health-ai-developer-foundations/medgemma)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xt2XZgaaH2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with [sufficient resources](https://ai.google.dev/gemma/docs/core#sizes) to run the MedGemma model.\n",
        "\n",
        "You can try out MedGemma 4B for free in Google Colab using a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "**Note**: To run the demo with MedGemma 27B in Google Colab, you will need a runtime with an A100 GPU and use 4-bit quantization to reduce memory usage. The performance of quantized versions has not been evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ITcQtdal7J"
      },
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-4b-it) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFQnPL2a9Dj"
      },
      "source": [
        "### Authenticate with Hugging Face\n",
        "\n",
        "Generate a Hugging Face `read` access token by going to [settings](https://huggingface.co/settings/tokens).\n",
        "\n",
        "If you are using Google Colab, add your access token to the Colab Secrets manager to securely store it. If not, proceed to run the cell below to authenticate with Hugging Face.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel. <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "2. Create a new secret with the name `HF_TOKEN`.\n",
        "3. Copy/paste your token key into the Value input box of `HF_TOKEN`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZwUUIY0gpY4W",
        "outputId": "8342d9fa-86f0-4b05-869c-1ac672f5fbeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret HF_TOKEN does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2608589376.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Use secret if running in Google Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Store Hugging Face data under `/content` if running in Colab Enterprise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HF_TOKEN does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "google_colab = \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\")\n",
        "\n",
        "if google_colab:\n",
        "    # Use secret if running in Google Colab\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "else:\n",
        "    # Store Hugging Face data under `/content` if running in Colab Enterprise\n",
        "    if os.environ.get(\"VERTEX_PRODUCT\") == \"COLAB_ENTERPRISE\":\n",
        "        os.environ[\"HF_HOME\"] = \"/content/hf\"\n",
        "    # Authenticate with Hugging Face\n",
        "    from huggingface_hub import get_token\n",
        "    if get_token() is None:\n",
        "        from huggingface_hub import notebook_login\n",
        "        notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7xTbWg6pY4W"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CulOXOrhpY4W"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet accelerate bitsandbytes transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRN9Yg_kpY4X"
      },
      "source": [
        "## Load model from Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YORs_sDfpY4X"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_variant = \"4b-it\"  # @param [\"4b-it\", \"27b-it\", \"27b-text-it\"]\n",
        "model_id = f\"google/medgemma-{model_variant}\"\n",
        "\n",
        "use_quantization = True  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Set `is_thinking` to `True` to turn on thinking mode. **Note:** Thinking is supported for the 27B variants only.\n",
        "is_thinking = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# If running a 27B variant in Google Colab, check if the runtime satisfies\n",
        "# memory requirements\n",
        "if \"27b\" in model_variant and google_colab:\n",
        "    if not (\"A100\" in torch.cuda.get_device_name(0) and use_quantization):\n",
        "        raise ValueError(\n",
        "            \"Runtime has insufficient memory to run a 27B variant. \"\n",
        "            \"Please select an A100 GPU and use 4-bit quantization.\"\n",
        "        )\n",
        "\n",
        "model_kwargs = dict(\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "if use_quantization:\n",
        "    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPhEFjiOTpcM"
      },
      "source": [
        "The following sections contain standalone examples demonstrating how to use the model both directly and with the [`pipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines) API. The `pipeline` API provides a simple way to use the model for inference while abstracting away complex details,  while directly using the model gives you complete control over the inference process, including preprocessing and postprocessing. In practice, you should select the method that is best suited for your use case.\n",
        "\n",
        "Here, you will load the model directly and with the `pipeline` API for use in the next sections. Note that the multimodal variants and the 27B text-only variant are loaded with their respective tasks and classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Of7LKuT_Sz"
      },
      "source": [
        "**Load model with the `pipeline` API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh1QcEXJT8zj"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "if \"text\" in model_variant:\n",
        "    pipe = pipeline(\"text-generation\", model=model_id, model_kwargs=model_kwargs)\n",
        "else:\n",
        "    pipe = pipeline(\"image-text-to-text\", model=model_id, model_kwargs=model_kwargs)\n",
        "\n",
        "pipe.model.generation_config.do_sample = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9F5HEr6UMqO"
      },
      "source": [
        "**Load model directly**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjGwhqdfUVI0"
      },
      "outputs": [],
      "source": [
        "if \"text\" in model_variant:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "else:\n",
        "    from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "    model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "    processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3M0Hyl3pY4X"
      },
      "source": [
        "## Run inference on images and text\n",
        "\n",
        "This section demonstrates running inference on image-based tasks using multimodal variants.\n",
        "\n",
        "**Note:** Proceed to [Run inference on text only](#scrollTo=tcyXG4lTpY4X) if you have selected the 27B text-only variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qunAkiKspY4X"
      },
      "outputs": [],
      "source": [
        "if \"text\" in model_variant:\n",
        "    raise ValueError(\n",
        "        \"You are using a text-only variant which does not support multimodal \"\n",
        "        \"inputs. Please proceed to the 'Run inference on text only' section.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-4LriNOpY4X"
      },
      "source": [
        "**Specify image and text inputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UterxS4WpY4X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from IPython.display import Image as IPImage, display, Markdown\n",
        "\n",
        "prompt = \"Describe this X-ray\"  # @param {type: \"string\"}\n",
        "\n",
        "# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"  # @param {type: \"string\"}\n",
        "! wget -nc -q {image_url}\n",
        "image_filename = os.path.basename(image_url)\n",
        "image = Image.open(image_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG-O7zKCpY4X"
      },
      "source": [
        "**Format conversation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgL2JLlGpY4X"
      },
      "outputs": [],
      "source": [
        "role_instruction = \"You are an expert radiologist.\"\n",
        "if \"27b\" in model_variant and is_thinking:\n",
        "    system_instruction = f\"SYSTEM INSTRUCTION: think silently if needed. {role_instruction}\"\n",
        "    max_new_tokens = 1300\n",
        "else:\n",
        "    system_instruction = role_instruction\n",
        "    max_new_tokens = 300\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": system_instruction}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image\", \"image\": image}\n",
        "        ]\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSTA4gRzpY4X"
      },
      "source": [
        "**Run model with the `pipeline` API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S8x3TYZpY4X"
      },
      "outputs": [],
      "source": [
        "output = pipe(text=messages, max_new_tokens=max_new_tokens)\n",
        "response = output[0][\"generated_text\"][-1][\"content\"]\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\"))\n",
        "display(IPImage(filename=image_filename, height=300))\n",
        "if \"27b\" in model_variant and is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"---\\n\\n**[ MedGemma thinking ]**\\n\\n{thought}\"))\n",
        "display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE18U9WppY4X"
      },
      "source": [
        "**Run the model directly**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EY1WFbhpY4X"
      },
      "outputs": [],
      "source": [
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "response = processor.decode(generation, skip_special_tokens=True)\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\"))\n",
        "display(IPImage(filename=image_filename, height=300))\n",
        "if \"27b\" in model_variant and is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"---\\n\\n**[ MedGemma thinking ]**\\n\\n{thought}\"))\n",
        "display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcyXG4lTpY4X"
      },
      "source": [
        "## Run inference on text only\n",
        "\n",
        "This section demonstrates running inference on text-based tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU1WbqUkpY4X"
      },
      "source": [
        "**Specify text prompt and format conversation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATUo4LDppY4X"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "prompt = \"How do you differentiate bacterial from viral pneumonia?\"  # @param {type: \"string\"}\n",
        "\n",
        "role_instruction = \"You are a helpful medical assistant.\"\n",
        "if \"27b\" in model_variant and is_thinking:\n",
        "    system_instruction = f\"SYSTEM INSTRUCTION: think silently if needed. {role_instruction}\"\n",
        "    max_new_tokens = 1500\n",
        "else:\n",
        "    system_instruction = role_instruction\n",
        "    max_new_tokens = 500\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": system_instruction}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IN8jQObpY4X"
      },
      "source": [
        "**Run model with the `pipeline` API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF9WVe5spY4X"
      },
      "outputs": [],
      "source": [
        "output = pipe(messages, max_new_tokens=max_new_tokens)\n",
        "response = output[0][\"generated_text\"][-1][\"content\"]\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\\n\\n---\"))\n",
        "if \"27b\" in model_variant and is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"**[ MedGemma thinking ]**\\n\\n{thought}\\n\\n---\"))\n",
        "display(Markdown(f\"**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n59HClOJpY4X"
      },
      "source": [
        "**Run the model directly**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyVj7lhKpY4X"
      },
      "outputs": [],
      "source": [
        "processor_or_tokenizer = tokenizer if \"text\" in model_variant else processor\n",
        "\n",
        "inputs = processor_or_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generation = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    generation = generation[0][input_len:]\n",
        "\n",
        "response = processor_or_tokenizer.decode(generation, skip_special_tokens=True)\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\\n\\n---\"))\n",
        "if \"27b\" in model_variant and is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"**[ MedGemma thinking ]**\\n\\n{thought}\\n\\n---\"))\n",
        "display(Markdown(f\"**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTxQttKYNpa"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medgemma/blob/main/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}